{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0782eb4a",
   "metadata": {},
   "source": [
    "#### Renaming Images in Dataset e.g.,1.ÿß.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761da858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Renamed all files in: ÿ®\n",
      "‚úÖ Renamed all files in: ÿØ\n",
      "‚úÖ Renamed all files in: ⁄æ\n",
      "‚úÖ Renamed all files in: €å\n",
      "‚úÖ Renamed all files in: ÿ°\n",
      "‚úÖ Renamed all files in: ŸÜ\n",
      "‚úÖ Renamed all files in: ÿ≥\n",
      "‚úÖ Renamed all files in: ŸÅ\n",
      "‚úÖ Renamed all files in: ÿ¥\n",
      "‚úÖ Renamed all files in: Ÿà\n",
      "‚úÖ Renamed all files in: ÿ∫\n",
      "‚úÖ Renamed all files in: ⁄Ü\n",
      "‚úÖ Renamed all files in: ÿß\n",
      "‚úÖ Renamed all files in: ÿÆ\n",
      "‚úÖ Renamed all files in: ⁄à\n",
      "‚úÖ Renamed all files in: Ÿπ\n",
      "‚úÖ Renamed all files in: ÿµ\n",
      "‚úÖ Renamed all files in: ÿ≤\n",
      "‚úÖ Renamed all files in: Ÿæ\n",
      "‚úÖ Renamed all files in: ŸÇ\n",
      "‚úÖ Renamed all files in: ⁄Ø\n",
      "‚úÖ Renamed all files in: ÿ∑\n",
      "‚úÖ Renamed all files in: ŸÖ\n",
      "‚úÖ Renamed all files in: ⁄ë\n",
      "‚úÖ Renamed all files in: ÿ∞\n",
      "‚úÖ Renamed all files in: ÿπ\n",
      "‚úÖ Renamed all files in: ⁄ò\n",
      "‚úÖ Renamed all files in: ÿ¨\n",
      "‚úÖ Renamed all files in: €Å\n",
      "‚úÖ Renamed all files in: ÿ´\n",
      "‚úÖ Renamed all files in: €í\n",
      "‚úÖ Renamed all files in: ÿ∏\n",
      "‚úÖ Renamed all files in: ŸÑ\n",
      "‚úÖ Renamed all files in: ÿ±\n",
      "‚úÖ Renamed all files in: ⁄©\n",
      "‚úÖ Renamed all files in: ÿ∂\n",
      "‚úÖ Renamed all files in: ÿ™\n",
      "‚úÖ Renamed all files in: ÿ≠\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Base Dataset folder ka path\n",
    "base_folder = \"../Dataset\"\n",
    "\n",
    "# Har Urdu letter folder ke liye loop\n",
    "for folder_name in os.listdir(base_folder):\n",
    "    folder_path = os.path.join(base_folder, folder_name)\n",
    "    \n",
    "    # Agar folder hai (file nahi)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for i, file in enumerate(os.listdir(folder_path), start=1):\n",
    "            old_path = os.path.join(folder_path, file)\n",
    "            ext = file.split('.')[-1]  # file extension (jpg/png)\n",
    "            new_name = f\"{i}.{folder_name}.{ext}\"  # e.g. 1.ÿß.jpg\n",
    "            new_path = os.path.join(folder_path, new_name)\n",
    "            \n",
    "            os.rename(old_path, new_path)\n",
    "        print(f\"‚úÖ Renamed all files in: {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbcb72",
   "metadata": {},
   "source": [
    "#### Preprocessing of Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646206b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Paths\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "DATASET_DIR = \"../Dataset\"\n",
    "PREPROCESSED_DIR = \"../Preprocessed\"\n",
    "\n",
    "# Target image size\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "# Make sure Preprocessed folder exists\n",
    "if not os.path.exists(PREPROCESSED_DIR):\n",
    "    os.makedirs(PREPROCESSED_DIR)\n",
    "\n",
    "# Process each Urdu letter folder\n",
    "for folder_name in os.listdir(DATASET_DIR):\n",
    "    folder_path = os.path.join(DATASET_DIR, folder_name)\n",
    "    save_folder = os.path.join(PREPROCESSED_DIR, folder_name)\n",
    "\n",
    "    # Skip non-folder items\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    # Create corresponding folder in Preprocessed\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    # Process all images inside the folder\n",
    "    for img_file in os.listdir(folder_path):\n",
    "        try:\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "            \n",
    "            if image is None:\n",
    "                print(f\"‚ö†Ô∏è Skipping unreadable file: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Resize to 64x64\n",
    "            image = cv2.resize(image, IMG_SIZE)\n",
    "            \n",
    "            # Normalize pixel values (0‚Äì1)\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Save preprocessed image\n",
    "            save_path = os.path.join(save_folder, img_file)\n",
    "            cv2.imwrite(save_path, (image * 255).astype(np.uint8))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Completed preprocessing for: {folder_name}\")\n",
    "\n",
    "print(\"\\nüéâ All images preprocessed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80766525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_edge_features(image):\n",
    "    # Resize and blur slightly to remove noise\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    image = cv2.GaussianBlur(image, (3, 3), 0)\n",
    "\n",
    "    # Compute gradients (edges)\n",
    "    gx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n",
    "    gy = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n",
    "\n",
    "    # Compute magnitude and angle\n",
    "    magnitude, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "\n",
    "    # Create a histogram of gradients\n",
    "    hist, _ = np.histogram(angle, bins=9, range=(0, 180), weights=magnitude)\n",
    "\n",
    "    # Normalize\n",
    "    hist = hist / np.linalg.norm(hist) if np.linalg.norm(hist) != 0 else hist\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35628f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully!\n",
      "Total images: 984\n",
      "Training images: 787\n",
      "Testing images: 197\n",
      "Classes: ['ÿ®', 'ÿØ', '⁄æ', '€å', 'ÿ°', 'ŸÜ', 'ÿ≥', 'ŸÅ', 'ÿ¥', 'Ÿà', 'ÿ∫', '⁄Ü', 'ÿß', 'ÿÆ', '⁄à', 'Ÿπ', 'ÿµ', 'ÿ≤', 'Ÿæ', 'ŸÇ', '⁄Ø', 'ÿ∑', 'ŸÖ', '⁄ë', 'ÿ∞', 'ÿπ', '⁄ò', 'ÿ¨', '€Å', 'ÿ´', '€í', 'ÿ∏', 'ŸÑ', 'ÿ±', '⁄©', 'ÿ∂', 'ÿ™', 'ÿ≠']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "classes = []\n",
    "\n",
    "dataset_path = \"../Preprocessed\"\n",
    "\n",
    "for idx, folder in enumerate(os.listdir(dataset_path)):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Skip non-folder items (like .DS_Store)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    classes.append(folder)\n",
    "\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            data.append(img)\n",
    "            labels.append(idx)\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "X = X / 255.0\n",
    "X = X.reshape(-1, 64, 64, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"Total images: {len(X)}\")\n",
    "print(f\"Training images: {len(X_train)}\")\n",
    "print(f\"Testing images: {len(X_test)}\")\n",
    "print(f\"Classes: {classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95adcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 984 images from 38 classes.\n",
      "üöÄ Training HOG+SVM model...\n",
      "‚úÖ HOG+SVM Model trained successfully! Accuracy: 47.21%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.67      0.27         3\n",
      "           1       0.33      0.17      0.22         6\n",
      "           3       1.00      0.11      0.20         9\n",
      "           5       0.50      0.67      0.57         3\n",
      "           6       0.33      0.50      0.40         4\n",
      "           7       0.50      0.80      0.62         5\n",
      "           8       1.00      1.00      1.00         4\n",
      "           9       0.67      0.33      0.44         6\n",
      "          10       0.17      0.50      0.25         4\n",
      "          11       0.60      0.33      0.43         9\n",
      "          12       0.86      1.00      0.92        12\n",
      "          13       0.40      0.40      0.40         5\n",
      "          14       0.56      0.62      0.59         8\n",
      "          15       0.17      0.33      0.22         3\n",
      "          16       0.29      0.50      0.36         4\n",
      "          18       0.75      0.38      0.50         8\n",
      "          19       0.00      0.00      0.00         1\n",
      "          20       0.75      0.43      0.55         7\n",
      "          21       0.00      0.00      0.00         6\n",
      "          22       0.88      0.88      0.88         8\n",
      "          23       0.45      0.56      0.50         9\n",
      "          24       0.25      0.25      0.25         8\n",
      "          25       0.33      0.20      0.25         5\n",
      "          26       0.33      0.12      0.18         8\n",
      "          27       0.38      0.50      0.43         6\n",
      "          29       0.00      0.00      0.00         6\n",
      "          30       0.80      1.00      0.89         4\n",
      "          31       0.33      0.50      0.40         4\n",
      "          32       0.60      0.75      0.67         8\n",
      "          33       1.00      0.40      0.57         5\n",
      "          34       0.33      0.33      0.33         6\n",
      "          35       1.00      1.00      1.00         6\n",
      "          36       0.25      0.25      0.25         4\n",
      "          37       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.47       197\n",
      "   macro avg       0.47      0.46      0.43       197\n",
      "weighted avg       0.53      0.47      0.46       197\n",
      "\n",
      "üíæ Model saved to 'models/hog_svm_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# ---------------------------------\n",
    "# üîß Step 1: Fixed HOG Feature Extractor\n",
    "# ---------------------------------\n",
    "def extract_hog_features(image):\n",
    "    image = cv2.resize(image, (64, 128))  # HOGDescriptor default expects 64x128\n",
    "\n",
    "    # Create HOG descriptor with proper parameters\n",
    "    hog = cv2.HOGDescriptor(\n",
    "        _winSize=(64, 128),\n",
    "        _blockSize=(16, 16),\n",
    "        _blockStride=(8, 8),\n",
    "        _cellSize=(8, 8),\n",
    "        _nbins=9\n",
    "    )\n",
    "    \n",
    "    h = hog.compute(image)\n",
    "    return h.flatten()\n",
    "\n",
    "# ---------------------------------\n",
    "# üìÇ Step 2: Load Dataset\n",
    "# ---------------------------------\n",
    "dataset_path = \"../Preprocessed\"\n",
    "data, labels = [], []\n",
    "classes = [f for f in os.listdir(dataset_path) if not f.startswith('.')]\n",
    "\n",
    "for idx, folder in enumerate(classes):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    for img_name in os.listdir(folder_path):\n",
    "        if img_name.startswith('.'):\n",
    "            continue\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            features = extract_hog_features(img)\n",
    "            data.append(features)\n",
    "            labels.append(idx)\n",
    "\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(X)} images from {len(classes)} classes.\")\n",
    "\n",
    "# ---------------------------------\n",
    "# ‚úÇÔ∏è Step 3: Train-Test Split\n",
    "# ---------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------------\n",
    "# üß† Step 4: Train SVM Model\n",
    "# ---------------------------------\n",
    "print(\"üöÄ Training HOG+SVM model...\")\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------\n",
    "# üìä Step 5: Evaluate\n",
    "# ---------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ HOG+SVM Model trained successfully! Accuracy: {acc*100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# ---------------------------------\n",
    "# üíæ Step 6: Save Model\n",
    "# ---------------------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(model, \"models/hog_svm_model.pkl\")\n",
    "print(\"üíæ Model saved to 'models/hog_svm_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c2181a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "# ------------------- 1Ô∏è‚É£ Imports -------------------\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------- 2Ô∏è‚É£ Paths -------------------\n",
    "original_path = \"Dataset\"\n",
    "augmented_path = \"Augmented_Dataset\"\n",
    "\n",
    "# ------------------- 3Ô∏è‚É£ Load Dataset -------------------\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(folder_path):\n",
    "        folder_full = os.path.join(folder_path, folder)\n",
    "        if not os.path.isdir(folder_full):\n",
    "            continue\n",
    "        for img_name in os.listdir(folder_full):\n",
    "            if img_name.startswith('.'):\n",
    "                continue\n",
    "            img_path = os.path.join(folder_full, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                print(\"Skipped invalid image:\", img_path)\n",
    "                continue\n",
    "            img = cv2.resize(img, (128,128))\n",
    "            _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            images.append(img)\n",
    "            labels.append(folder)\n",
    "    return images, labels\n",
    "\n",
    "print(\"Loading original dataset...\")\n",
    "orig_images, orig_labels = load_images_from_folder(original_path)\n",
    "print(\"Loading augmented dataset...\")\n",
    "aug_images, aug_labels = load_images_from_folder(augmented_path)\n",
    "\n",
    "# Combine datasets\n",
    "all_images = orig_images + aug_images\n",
    "all_labels = orig_labels + aug_labels\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(all_images, dtype=np.float32)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Normalize\n",
    "X = X / 255.0\n",
    "\n",
    "# Flatten images for MLP\n",
    "X = X.reshape(X.shape[0], -1)  # 128*128 = 16384 features\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_enc = encoder.fit_transform(y)\n",
    "y_cat = to_categorical(y_enc)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_cat, test_size=0.1, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# ------------------- 4Ô∏è‚É£ Build ANN Model -------------------\n",
    "num_classes = y_cat.shape[1]\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ------------------- 5Ô∏è‚É£ Train Model -------------------\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=50,\n",
    "                    batch_size=32)\n",
    "\n",
    "# ------------------- 6Ô∏è‚É£ Evaluate Model -------------------\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ------------------- 7Ô∏è‚É£ Predict Single Image -------------------\n",
    "def predict_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(\"Invalid image path\")\n",
    "        return\n",
    "    img = cv2.resize(img, (128,128))\n",
    "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    img = img.astype(np.float32)/255.0\n",
    "    img_flat = img.reshape(1,-1)\n",
    "    pred = model.predict(img_flat)\n",
    "    label = encoder.inverse_transform([np.argmax(pred)])\n",
    "    print(\"Predicted Letter:\", label[0])\n",
    "\n",
    "# Example Usage\n",
    "# predict_image('Dataset/ÿß/img1.jpg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
